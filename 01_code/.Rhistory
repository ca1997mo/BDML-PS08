`01_main_data` <- readRDS("C:/Users/sanyl/OneDrive - Universidad de los Andes/Documentos/2.PEG-UNIANDES/7.Bigdata_MachineL/PSET_1_BDML_2025_02_G2/02_prepare_data/03_output/01_main_data.rds")
#======================#
# Sany Leon
# CDATE:  02/01/2026
# MDATE:  02/01/2026
#======================#
# setup
rm(list = ls())
pacman::p_load(rio, rvest, tidyverse, data.table)
url <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
#--- Data
page <- read_html(url)
# obtener links internos de pagina principal
links <- page %>%
html_elements("a") %>%
html_attr("href") %>%
c() %>%
.[str_detect(string = ., pattern = "page")] %>%
paste0(url,.)
# ingresar a pagina y hacer scrapping
data = map(.x = links, .f = function(x){
Sys.sleep(5)
link = read_html(x) %>%
as.character(page) %>%
str_extract_all(pattern = "pages/geih_page.+\\.html") %>%
unlist() %>%
paste0(url, .)
# importar datos
data = read_html(link) %>%
html_table()
return(data[[1]])
})
data = rbindlist(data)
#--- dictionaries
dict = read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/dictionary.html") %>%
html_table() %>%
.[[1]]
labels = read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/labels.html") %>%
html_table() %>%
.[[1]]
dictionary = list("dictionarie" = dict,
"labels" = labels)
export(data, "data_output/01_data_scrapping_web_page.rds")
export(dictionary, "dictionary.xlsx")
export(dictionary, "dictionary.xlsx")
View(dictionary)
View(dictionary)
View(labels)
View(page)
View(data)
`01_data_scrapping_web_page` <- readRDS("C:/Users/sanyl/OneDrive - Universidad de los Andes/Documentos/2.PEG-UNIANDES/7.Bigdata_MachineL/PS1_2026/BDML-PS08/01_code/data_output/01_data_scrapping_web_page.rds")
